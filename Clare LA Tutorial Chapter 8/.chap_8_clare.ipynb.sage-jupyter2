{"backend_state":"ready","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-d98566ae-1c7c-4ac3-b34c-8e8007b8c62e.json","kernel":"sage-10.1","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1709967754382,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1707589228499,"exec_count":3,"id":"5d8686","input":"u=vector([2,-1,5,-2])\nv=vector([3,2,-4,0])\nw=vector([2,9,6,4])\n\n# computing dot products\nprint(\"u\\u2022v = \", u.dot_product(v))\nprint(\"v\\u2022w = \", v.dot_product(w), \"\\n\")\n\nprint(\"||v|| = \", v.norm())","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"u•v =  -16\nv•w =  0 \n\n||v|| =  sqrt(29)\n"}},"pos":2,"start":1707589228386,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589230717,"exec_count":4,"id":"10aeef","input":"zero = vector([0,0,0])\nu=vector([2,-1,-3])\nv=vector([3,2,-4])\nw=vector([2,9,6])\n\n# compute dot products and visualize normalized vectors\nprint(\"u\\u2022v = \", u.dot_product(v))\nprint(\"v\\u2022w = \", v.dot_product(w))\n\n# normalizing scales the vectors to magnitude 1\nplot = (arrow(zero, u.normalized(), color=\"red\") + \n    arrow(zero, v.normalized(), color=\"purple\") + arrow(zero, w.normalized()))\nplot.show()","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"u•v =  16\nv•w =  0\n"},"1":{"data":{"iframe":"c25cc7258014ea2b13bbda1a48dd69bbe3a0a949","text/plain":"Graphics3d Object"}}},"pos":3,"start":1707589230027,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589234124,"exec_count":5,"id":"d037ec","input":"# .norm() comutes the magnitude (norm) of the vector\nprint(\"Pathagorian Theorem: \", (v+w).norm()^2, \"=\", v.norm()^2 + w.norm()^2)\nprint(\"Triangle Inequality: \", round((u+v).norm(), 3), \"<\", round(u.norm() + v.norm(),3))","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"Pathagorian Theorem:  150 = 150\nTriangle Inequality:  8.66 < 9.127\n"}},"pos":5,"start":1707589234116,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589237120,"exec_count":6,"id":"e5ce71","input":"u_1 = vector([1,2,0,-1])\nu_2 = vector([5,2,4,9])\nu_3 = vector([-2,2,-3,2])\n\n# checks that the dot product of all vectors in basis equal 0.\nif u_1.dot_product(u_2) == u_2.dot_product(u_3) == u_1.dot_product(u_3) == 0:\n    print(\"The basis {u_1, u_2, u_3} are an orthogonal basis for span{u_1, u_2, u_3}.\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"The basis {u_1, u_2, u_3} are an orthogonal basis for span{u_1, u_2, u_3}.\n"}},"pos":7,"start":1707589237116,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589238326,"exec_count":7,"id":"559063","input":"s = vector([19,4,17,51])    # s is in span{u_1,u_2,u_3}\n\n# Find coeffcients to express s as a linear combination of {u_1, u_2, u_3}\nc_1 = s.dot_product(u_1)/u_1.norm()^2\nc_2 = s.dot_product(u_2)/u_2.norm()^2\nc_3 = s.dot_product(u_3)/u_3.norm()^2\n\nif s == c_1*u_1 + c_2*u_2 + c_3*u_3:\n    print(\"As expected: \", s, \" = \", c_1, u_1, \" + \", c_2, u_2, \" + \", c_3, u_3)","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"As expected:  (19, 4, 17, 51)  =  -4 (1, 2, 0, -1)  +  5 (5, 2, 4, 9)  +  1 (-2, 2, -3, 2)\n"}},"pos":8,"start":1707589238313,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589240518,"exec_count":8,"id":"cd1d33","input":"def Gram_Schmidt(B):\n    \"\"\"\n    This function take in an orthogonal basis, B, for\n    a subspace and outputs the corresponding orthogonal\n    basis for the subspace using the Gram-Schmidt process.\n    (Note: This function does not handle errors)\n    \n    Inputs:\n    -------\n    B : List of vectors\n    \n    Outputs:\n    --------\n    O : List of vectors\n    \"\"\"\n    O = []\n    for i, v_i in enumerate(B):\n        orth_v = v_i - sum([(v_i.dot_product(O[j])/O[j].norm()^2)*O[j] for j in range(i)])    # utilizing projection formula\n        O.append(orth_v)\n        \n    return O","kernel":"sage-10.1","pos":10,"start":1707589240511,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589241657,"exec_count":9,"id":"e72fee","input":"B = [vector([1,0,1,1]), vector([0,2,0,3]), vector([-3,-1,1,5])]\nGram_Schmidt(B)\n\n# try this yourself using a larger basis that would be too difficult to compute by hand!\n# B = []","kernel":"sage-10.1","output":{"0":{"data":{"text/plain":"[(1, 0, 1, 1), (-1, 2, -1, 2), (-3, -3, 1, 2)]"},"exec_count":9}},"pos":11,"start":1707589241648,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589244082,"exec_count":10,"id":"d2e70f","input":"def orthogonal_diagonalization(A):\n    \"\"\"\n    This function takes in a symmetric matrix, A, and\n    outputs orthogonal and diagonal matrices which\n    orthogonally diagonalize A.\n    \n    Inputs: \n    -------\n    A : matrix (symmetric)\n    \n    Outputs:\n    --------\n    matrix: A diagonal matrix \n    matrix: an orthogonal (optionally orthonormal) matrix\n    \"\"\"\n    if A.transpose() != A:\n        raise ValueError(\"orthogonal_diagonalization only accepts symmetric matrices\")\n        \n    O = []\n    \n    # sort to look at eigen vectors from largest to smallest eigenvalue\n    # this will be important when using (SVD)\n    for _, eigen_vectors, num_vectors in sorted(A.eigenvectors_right(), reverse=True):\n        if num_vectors > 1:\n            O.extend(Gram_Schmidt(eigen_vectors))    # if an eigen value has multiplicity, we need to \n                                                         # find an orthogonal basis for the multiple eigen vectors.\n        else:\n            O.extend(eigen_vectors)\n    \n    # normalize matrix vectors\n    norm_O = []\n    for vector in O:\n        norm_O.append(vector.normalized())\n        \n    return matrix.diagonal(sorted(A.eigenvalues(), reverse=True)), matrix(norm_O).transpose()","kernel":"sage-10.1","pos":13,"start":1707589244076,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589250218,"exec_count":11,"id":"48178d","input":"A = matrix(SR, [[1,3,-3,-3],[3,-3,3,-1],[-3,3,1,-3],[-3,-1,-3,-3]])\n\nD, P = orthogonal_diagonalization(A)\n\nif A == P*D*P.inverse():\n    print(A, \" is orthogonally diagonalized by the orthogonal matrix\\n\\n\", P, \" and the diagonal matrix\\n\\n\", D)\n    \n# try this yourself with your own symmetric matrix!\n# A = matrix([[]])","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"[ 1  3 -3 -3]\n[ 3 -3  3 -1]\n[-3  3  1 -3]\n[-3 -1 -3 -3]  is orthogonally diagonalized by the orthogonal matrix\n\n [ 1/2*sqrt(2)          1/2            0          1/2]\n[           0          1/2  1/2*sqrt(2)         -1/2]\n[-1/2*sqrt(2)          1/2            0          1/2]\n[           0         -1/2  1/2*sqrt(2)          1/2]  and the diagonal matrix\n\n [ 4  0  0  0]\n[ 0  4  0  0]\n[ 0  0 -4  0]\n[ 0  0  0 -8]\n"}},"pos":14,"start":1707589245366,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589258533,"exec_count":12,"id":"a320ed","input":"def QR_factorization(A):\n    \"\"\"\n    This function takes in a matrix, A, with linearly indep.\n    columns and outputs orthonormal and upper triangular matrices \n    which QR factorize A.\n    \n    Inputs: \n    -------\n    A : matrix (linearly indpendent columns)\n    \n    Outputs:\n    --------\n    matrix : orthonormal matrix \n    matrix: upper triangular matrix with positive diagonal\n    \"\"\"\n    norm_O = [v.normalized() for v in Gram_Schmidt(A.columns())]    # normalize orthogonal basis\n    Q_transpose = matrix(norm_O)    # when defining Q with column vectors, the matrix returned is the transpose of Q\n    \n    return Q_transpose.transpose(), Q_transpose*A","kernel":"sage-10.1","pos":16,"start":1707589258530,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589259763,"exec_count":13,"id":"4fe5b1","input":"# Note: matrix A uses the same vectors as the previous Gram-Schmidt example.\nA = matrix([[1,0,-3],[0,2,-1],[1,0,1],[1,3,5]])\n\nQ, R = QR_factorization(A)\n\nif A == Q*R:\n    print(A, \" is QR factorized by the orthonormal matrix\\n\\n\", Q, \" and the upper triangular matrix with positive diagonal matrix\\n\\n\", R)\n        \n# try this yourself with your own matrix!\n# A = matrix([])","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"[ 1  0 -3]\n[ 0  2 -1]\n[ 1  0  1]\n[ 1  3  5]  is QR factorized by the orthonormal matrix\n\n [   1/3*sqrt(3) -1/10*sqrt(10) -3/23*sqrt(23)]\n[             0   1/5*sqrt(10) -3/23*sqrt(23)]\n[   1/3*sqrt(3) -1/10*sqrt(10)  1/23*sqrt(23)]\n[   1/3*sqrt(3)   1/5*sqrt(10)  2/23*sqrt(23)]  and the upper triangular matrix with positive diagonal matrix\n\n [ sqrt(3)  sqrt(3)  sqrt(3)]\n[       0 sqrt(10) sqrt(10)]\n[       0        0 sqrt(23)]\n"}},"pos":17,"start":1707589259746,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589262415,"exec_count":14,"id":"946411","input":"def SVD(B):\n    \"\"\"\n    This function takes in any n x m matrix, B, and outputs an orthonormal n x n matrix,\n    a padded diagonal matrix and another orthonormal m x m matrix which factorize B.\n    \n    Inputs: \n    -------\n    B : matrix\n    \n    Outputs:\n    --------\n    matrix : n x n orthonormal matrix\n    matrix : diagonal matrix padded to be n x m\n    matrix : m x m orthonormal matrix\n    \"\"\"\n    # apply algorithm on matrix with at least as many rows as columns\n    if B.nrows() < B.ncols():\n        A = B.transpose()\n    else:\n        A = B\n       \n    D,V = orthogonal_diagonalization(A.transpose()*A)\n    root_D = D.principal_square_root()\n    \n    # pad diagonal matrix\n    S = root_D.stack(matrix(0, nrows=A.nrows()-A.ncols(), ncols=A.ncols()))\n        \n    U = []    \n    for i, s in enumerate(S.diagonal()):\n        U.append((A*V.column(i)/s).normalized())\n    for v in A.kernel().basis():\n        U.append(v.normalized())\n    \n    # return based on whether functoin ran on transpose\n    if A == B:\n        return matrix(U).transpose(), S, V.transpose()\n    else:\n        return V, S.transpose(), matrix(U)","kernel":"sage-10.1","pos":19,"start":1707589262411,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589263703,"exec_count":15,"id":"fb3103","input":"A = matrix([[1,2],[2,0],[0,2]])\nU, S, V_transpose = SVD(A)\n\nprint(A, \" = A\\n\")\nprint(U*S*V_transpose, \" = USV^T\\n\")\n\nB = matrix([[1,2,1,0],[2,0,1,1]])\nU, S, V_transpose = SVD(B)\n\nprint(B, \" = B\\n\")\nprint(U*S*V_transpose, \" = USV^T\\n\")\n\n# try this yourself with your own matrix!\n# C = matrix([])","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"[1 2]\n[2 0]\n[0 2]  = A\n\n[1 2]\n[2 0]\n[0 2]  = USV^T\n\n[1 2 1 0]\n[2 0 1 1]  = B\n\n[1 2 1 0]\n[2 0 1 1]  = USV^T\n\n"}},"pos":20,"start":1707589263582,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589270602,"exec_count":16,"id":"6fd229","input":"# import image into array with appropriate libraries\nfrom PIL import Image\nimport numpy as np\nfrom matplotlib.pyplot import imshow\n\nimg = Image.open(\"egg.png\").convert(\"L\")\negg_array = np.array(img)","kernel":"sage-10.1","pos":22,"start":1707589270366,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589281434,"exec_count":17,"id":"99ce5a","input":"# Use numpy's SVD algorithm and compute\nU,S,V_T = np.linalg.svd(egg_array, full_matrices=True)","kernel":"sage-10.1","pos":23,"start":1707589275203,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589291893,"exec_count":19,"id":"ce4b3e","input":"print(\"Original:\")\nimshow(egg_array, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"Original:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3d495410>"},"exec_count":19},"2":{"data":{"image/png":"b98c44c2ef34d70525a9a03db527c219aa1f5f88","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":24,"start":1707589291337,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589297289,"exec_count":20,"id":"0f1bb4","input":"print(\"SVD compressed 100:\")\nimshow(arr_100, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 100:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3d50d2d0>"},"exec_count":20},"2":{"data":{"image/png":"e9cacb830148036dfe4844bcb0664461def23eff","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":25,"start":1707589296473,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589315794,"exec_count":21,"id":"69367f","input":"print(\"SVD compressed 50:\")\nimshow(arr_50, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 50:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3c38d2d0>"},"exec_count":21},"2":{"data":{"image/png":"1f9e4a23085640888d13d3b1d6574da2b4e9499e","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":26,"start":1707589315188,"state":"done","type":"cell"}
{"cell_type":"code","end":1707589320431,"exec_count":22,"id":"cb78d3","input":"print(\"SVD compressed 10:\")\nimshow(arr_10, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 10:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3d509410>"},"exec_count":22},"2":{"data":{"image/png":"d9e6a8b85a58023cebe2f0fb40dd55b232a01d77","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":27,"start":1707589319588,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3226b6","input":"<span style='font-size:medium'>SVD has applications in image compression. We can express an </span>$n \\times m$<span style='font-size:medium'> matrix, </span>$A$<span style='font-size:medium'>, with the following sum </span>\n$$ \nA = \\alpha_1u_1v_1 + \\alpha_2u_2v_2 + \\cdots \\alpha_nu_nv_n\n$$\n<span style='font-size:medium'>where </span>$u_i$<span style='font-size:medium'> is a column vector of </span>$U$<span style='font-size:medium'>, </span>$v_i$<span style='font-size:medium'> is a row vector of </span>$V^T$<span style='font-size:medium'>, and the </span>$\\alpha_1\\ge \\alpha_2 \\ge \\cdots \\ge \\alpha_n$<span style='font-size:medium'> come from the diagonal of </span>$\\Sigma$<span style='font-size:medium'> in the </span>$SVD$<span style='font-size:medium'> decomposition of </span>$A$<span style='font-size:medium'>. The product used between $u_i$ and $v_i$ in the expansion is the outer product. Since the </span>$\\alpha_i$<span style='font-size:medium'> are decreasing, taking the sum with only the first </span>$k$<span style='font-size:medium'> terms  can still give us a good approximation of the original matrix. See section 8.4 \\[1\\] for more details. We show the results of this application below utilizing python's numpy library.</span>\n","pos":21,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"48dd1e","input":"<span style='font-size:medium'>An even more general factorization is singular value decomposition \\(SVD\\). SVD can be applied to any </span>$n\\times m$<span style='font-size:medium'> matrix and decomposes the matrix into </span>$U\\Sigma V^T$<span style='font-size:medium'> where </span>$U$<span style='font-size:medium'> is an </span>$n\\times n$<span style='font-size:medium'> orthogonal matrix, </span>$\\Sigma$<span style='font-size:medium'> is a diagonal matrix padded with 0's, and </span>$V$<span style='font-size:medium'> is an </span>$m\\times m$<span style='font-size:medium'> orthogonal matrix. More specific details on the decomposition can be found in section 8.4 \\[1\\]. The following function implements \\(SVD\\) for any </span>$n\\times m$<span style='font-size:medium'> matrix.</span>\n","pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"74a304","input":"<span style='font-size:medium'>In the more general case where we have an </span>$n\\times m$<span style='font-size:medium'> matrix </span>$A$<span style='font-size:medium'> with linearly independent columns, we can factorize by </span>$A = QR$<span style='font-size:medium'> where </span>$Q$<span style='font-size:medium'> is an orthonormal </span>$n\\times m$<span style='font-size:medium'> matrix and </span>$R$<span style='font-size:medium'> is an </span>$m\\times m$<span style='font-size:medium'> upper triangular matrix with positive diagonal entires. This method is known as QR Factorization. The following function takes a matrix with linearly independent columns and outputs the corresponding orthonormal and upper triangular matrices. See section 8.3 \\[1\\] for more details.</span>\n","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"ceeade","input":"# Chapter 8: Orthogonality\n## Originally created by Clare Minnerath\n\n**This Jupyter notebook is a part of a linear algebra tutorial created by Sara Billey's 2024 WXML group. It follows Chapter 8 of  Holt's Linear Algebra with applications. [1] which explores orthogonality and new ways to decompose a matrix into more easily understood chunks. It is intended as a supplement to the UW Math 208 course. The user is encouraged to explore beyond the scope of this notebook as interested.**\n\nNote: This notebook primarily uses Sage linear algebra libraries. However, the Single Value Decomposition (SVD) application to image compression utiilizes python's numpy library. Also, methods such as Gram-Schmidt and QR factorization are available built in as Sage functions, but functions are written out in this tutorial to be compared with the text's algorithms.\n\n<span> 1. Holt, J. \\(2017\\)._Linear algebra with applications_.</span>\\\n<span> 2. Cat image from Clare Minnerath: image of family cat Egg. </span>\\\n<span> 3. Dog image from Sara Billey: image of painting by Sara's uncle of her dog Valentine. </span>","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e414a3","input":"<span style='font-size:medium'>Since the Pathagorean Theorem only holds for right triangles, it follows that </span>$||u+v||^2 = ||u||^2 + ||v||^2$<span style='font-size:medium'> if and only if </span>$u\\cdot v=0$<span style='font-size:medium'>. The Triangle Inequality, </span>$||u+v|| \\le ||u|| + ||v||$<span style='font-size:medium'>, holds for all vectors </span>$u$<span style='font-size:medium'> and </span>$v$<span style='font-size:medium'>.</span>\n","pos":4,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e43f83","input":"<span style='font-size:medium'>An orthogonal matrix is a square matrix with unit length mutually orthogonal columns as well as unit length mutually orthogonal rows. A matrix, </span>$A$<span style='font-size:medium'>, is orthogonally diagonalizable if there exists an orthogonal matrix, </span>$P$<span style='font-size:medium'>, and a diagonal matrix, </span>$D$<span style='font-size:medium'>, such that </span>$A = PDP^{-1}$<span style='font-size:medium'>. The Spectral Theorem from section 8.3 \\[1\\], tells us that a matrix is orthogonally diagonalizable if and only if the matrix is symmetric. What a result! The following function takes a symmetric matrix and outputs corresponding orthogonal and diagonal matrices. In particular, this function will make use of the previous Gram\\-Schmidt process.</span>\n","pos":12,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e7fc62","input":"<span style='font-size:medium'>Two vectors in </span>$\\R^n$<span style='font-size:medium'> are orthogonal if their dot product is equal to 0. </span>\n<span style='font-size:medium'>The magnitude (or norm) of a vector is given by </span>$||x|| = \\sqrt{x\\cdot x}$<span style='font-size:medium'>.</span>\n","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"eea29d","input":"<span style='font-size:medium'>The Gram\\-Schmit Process is a method for transforming a basis for a subspace into an orthogonal basis for that subspace. The following function performs said algorithm, but more details can be found in section 8.2 of \\[1\\].</span>\n","pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"eeb5cd","input":"<span style='font-size:medium'>An orthogonal basis, </span>$\\{v_1,...,v_n\\}$<span style='font-size:medium'>, is a basis for a subspace such that all of the basis vectors are orthogonal \\(</span>$v_i\\cdot v_j=0$<span style='font-size:medium'> whenever </span>$i\\ne j$<span style='font-size:medium'>\\). Given such a basis, any vector, </span>$s$<span style='font-size:medium'>, in the subspace can be expressed as </span>$$s = c_1v_1+\\cdots + c_kv_k$$<span style='font-size:medium'> where </span>$c_i = \\frac{v_i\\cdot s}{||v_i||^2}$<span style='font-size:medium'>. Recalling that </span>$ \\frac{v\\cdot s}{||v||^2}v = \\text{proj}_vs$, we can also express the formula as $$s =  \\text{proj}_{v_1}s + \\text{proj}_{v_2}s + \\cdots + \\text{proj}_{v_k}s.$$\n","pos":6,"state":"done","type":"cell"}
{"end":1707589287919,"exec_count":18,"id":"0e3dbe","input":"# For 100, 50, 10 terms used, compute the compressed image \narr_100 = np.zeros(shape=egg_array.shape)\nfor i in range(100):\n    arr_100 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n\narr_50 = np.zeros(shape=egg_array.shape)\nfor i in range(50):\n    arr_50 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n    \narr_10 = np.zeros(shape=egg_array.shape)\nfor i in range(10):\n    arr_10 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n \nprint(\"Values for alpha at 1000, 100, 50, and 10th terms respectively: \", round(S[1000],3), round(S[100],3), round(S[50],3), round(S[10],3))","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"Values for alpha at 1000, 100, 50, and 10th terms respectively:  26.811 2119.213 3628.81 11689.741\n"}},"pos":23.5,"start":1707589283925,"state":"done","type":"cell"}
{"end":1707589326851,"exec_count":23,"id":"8147b0","input":"img = Image.open(\"val_gray.png\").convert(\"L\")\nval_array = np.array(img)","kernel":"sage-10.1","pos":29,"start":1707589326845,"state":"done","type":"cell"}
{"end":1707589328634,"exec_count":24,"id":"0a5221","input":"# Use numpy's SVD algorithm and compute\nU,S,V_T = np.linalg.svd(val_array, full_matrices=True)\n\n# For 50, 20, 10, 5 terms used, compute the compressed image \narr_50 = np.zeros(shape=val_array.shape)\nfor i in range(50):\n    arr_50 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n\narr_20 = np.zeros(shape=val_array.shape)\nfor i in range(20):\n    arr_20 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n    \narr_10 = np.zeros(shape=val_array.shape)\nfor i in range(10):\n    arr_10 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n    \narr_5 = np.zeros(shape=val_array.shape)\nfor i in range(5):\n    arr_5 += np.multiply.outer(U[:, i], V_T[i, :])*S[i]\n    \nprint(\"Values for alpha at 200, 50, 20, 10, and 5th terms respectively: \", round(S[200],3), round(S[50],3), round(S[20],3), round(S[10],3), round(S[5],3))","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"Values for alpha at 200, 50, 20, 10, and 5th terms respectively:  3.022 165.809 379.725 626.37 1215.939\n"}},"pos":30,"start":1707589328603,"state":"done","type":"cell"}
{"end":1707589336993,"exec_count":26,"id":"f8534f","input":"print(\"Original:\")\nimshow(val_array, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"Original:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3c43a5d0>"},"exec_count":26},"2":{"data":{"image/png":"1b9707356fc242f89e03f075ac547dc9053f2986","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":32,"start":1707589336830,"state":"done","type":"cell"}
{"end":1707589341014,"exec_count":27,"id":"cb720b","input":"print(\"SVD compressed 50:\")\nimshow(arr_50, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 50:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3d5225d0>"},"exec_count":27},"2":{"data":{"image/png":"03126c6bebc9ab2426d9be7366d0b13e5d85608c","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":33,"start":1707589340818,"state":"done","type":"cell"}
{"end":1707589345157,"exec_count":28,"id":"33e0b9","input":"print(\"SVD compressed 20:\")\nimshow(arr_20, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 20:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3be0d2d0>"},"exec_count":28},"2":{"data":{"image/png":"8cd06a2bee9340f3bc600744884126e7fcbcb926","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":34,"start":1707589344992,"state":"done","type":"cell"}
{"end":1707589349462,"exec_count":29,"id":"ce3c7c","input":"print(\"SVD compressed 10:\")\nimshow(arr_10, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 10:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd3be68390>"},"exec_count":29},"2":{"data":{"image/png":"687395ab331fc9be4ce39c5338f8a1ba928214ba","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":35,"start":1707589349276,"state":"done","type":"cell"}
{"end":1707589353789,"exec_count":30,"id":"f7e6ea","input":"print(\"SVD compressed 5:\")\nimshow(arr_5, cmap=\"gray\")","kernel":"sage-10.1","output":{"0":{"name":"stdout","text":"SVD compressed 5:\n"},"1":{"data":{"text/plain":"<matplotlib.image.AxesImage object at 0x7fbd39577390>"},"exec_count":30},"2":{"data":{"image/png":"bf087ebed31e3fa072632a0b39e29506655d2934","text/plain":"<Figure size 640x480 with 1 Axes>"}}},"pos":36,"start":1707589353650,"state":"done","type":"cell"}
{"id":0,"time":1709963757594,"type":"user"}
{"last_load":1706911388942,"type":"file"}